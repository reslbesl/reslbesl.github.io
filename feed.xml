<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://reslbesl.github.io/reslbesl/feed.xml" rel="self" type="application/atom+xml"/><link href="https://reslbesl.github.io/reslbesl/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-06-19T09:55:46+00:00</updated><id>https://reslbesl.github.io/reslbesl/feed.xml</id><title type="html">Theresa Stadler</title><subtitle>Personal website by Theresa Stadler. </subtitle><entry><title type="html">On the Fundamental Limits of Privacy-Enhancing Technologies</title><link href="https://reslbesl.github.io/reslbesl/blog/2024/phd/" rel="alternate" type="text/html" title="On the Fundamental Limits of Privacy-Enhancing Technologies"/><published>2024-11-01T00:00:00+00:00</published><updated>2024-11-01T00:00:00+00:00</updated><id>https://reslbesl.github.io/reslbesl/blog/2024/phd</id><content type="html" xml:base="https://reslbesl.github.io/reslbesl/blog/2024/phd/"><![CDATA[<p>On 1st November 2024, I defended my PhD thesis titled “On the fundamental limits of privacy-enhancing technologies”. The thesis is publicly available via the <a href="https://infoscience.epfl.ch/entities/publication/0ada1ede-9af8-4de1-b534-99cc0a63af27">EPFL library</a> and a short summary of its contributions can be found in the abstract below.</p> <p><strong>Abstract</strong></p> <p>The advancement of data-driven decision making has become a key priority in political agendas and private sector strategies. However, the growing demand for data and the opacity of data-driven systems raise concerns about the threats to individual privacy these systems pose. Privacy-enhancing technologies (PETs) are frequently promoted as a technological solution to address this tension between data use and privacy risks. Many suggest that PETs might mitigate privacy risks at little to no cost in data utility or system functionality.</p> <p>This thesis critically examines such claims and explores the fundamental limits of PETs. We uncover inherent trade-offs of PETs across the three distinct domains: synthetic data as a tool for privacy-preserving data publishing, least-privilege learning in machine learning as a service (MLaaS) settings, and privacy-preserving digital proximity tracing (DPT) systems. In each case, we demonstrate how a rigorous problem formalisation is crucial to understand the fundamental limits of PETs. We show that some trade-offs between intended data use and unintended information leakage are unavoidable and that certain privacy risks are inherent to a system’s core functionality.</p> <p>First, we investigate synthetic data as a PET. Synthetic data is frequently advertised as a silver-bullet solution to privacy-preserving microdata publishing that addresses the shortcomings of traditional data anonymisation. Contrary to these claims, our research reveals that high-quality synthetic data inevitably leaks sensitive information about individuals while strong privacy guarantees result in significant utility loss.</p> <p>Next, we formalise the concept of least-privilege representation learning in MLaaS settings and characterise its feasibility. We prove that there is a fundamental trade-off between a representation’s utility for a given task and its leakage beyond the intended task: it is not possible to learn representations that have high utility for the intended task but at the same time prevent inference of any attribute other than the task label itself. Our findings challenge the notion that least-privilege learning provides a promising avenue to mitigate the harms of potential data misuse in MLaaS settings.</p> <p>Finally, we present an in-depth privacy analysis of DPT systems. We show that certain risks are inherent to the core functionality of these systems and cannot be mitigated through design choices. Our analysis highlights the need to identify and assess privacy risks at the earliest stages of the system design process.</p> <p>Across all three domains, our research consistently reveals how PETs operate within fixed boundaries defined by their fundamental trade-offs. Our findings challenge the often overly optimistic portrayal of PETs as a technological cure-all for privacy concerns in data-driven innovation. This thesis argues for a new approach to the evaluation and design of data-driven privacy technologies and systems. It emphasises the importance to rigorously formalise and analyse claims about the potential benefits of PETs and to carefully evaluate inherent risks.</p> <p>Overall, this thesis contributes to a more realistic vision of PETs, in particular, of their capabilities and limitations to enable the privacy-preserving use of data. By acknowledging the fundamental limits of PETs, our research aims to foster a more responsible approach to innovation in data-driven technologies.</p>]]></content><author><name></name></author><category term="privacy"/><summary type="html"><![CDATA[Public defence of my PhD thesis]]></summary></entry><entry><title type="html">Privacy-preserving Data Sharing and the European Data Strategy</title><link href="https://reslbesl.github.io/reslbesl/blog/2024/brussels/" rel="alternate" type="text/html" title="Privacy-preserving Data Sharing and the European Data Strategy"/><published>2024-08-25T00:00:00+00:00</published><updated>2024-08-25T00:00:00+00:00</updated><id>https://reslbesl.github.io/reslbesl/blog/2024/brussels</id><content type="html" xml:base="https://reslbesl.github.io/reslbesl/blog/2024/brussels/"><![CDATA[<p>It was a great pleasure to give a lecture on the fundamental limits of privacy-enhancing data sharing technologies at the <a href="https://brusselsprivacyhub.com/education-training/summer-academy-for-global-privacy-law-2024/">Brussels Privacy Hub Global Summer Academy 2024</a>.</p> <p>The main question that I tried to answer during my lecture is whether privacy-enhancing technologies can achieve their envisioned role in a future data-driven economy as outlined by the <a href="https://digital-strategy.ec.europa.eu/en/policies/strategy-data">Europen Data Strategy</a> and similiar legislation and regulatory frameworks.</p> <p>The <strong>key take-aways</strong> of the lecture were</p> <p><strong>Any data sharing use case brings with it an inherent risk that is defined by its intended purpose.</strong></p> <ul> <li> <p>Every data use case comes with inherent privacy risks. Inherent risks are privacy risks that are intrinsically linked to the intended data use which is defined by the purpose of the data sharing.</p> </li> <li> <p>If we analyse the envisioned purposes of data sharing outlined by the European Data Strategy and other relevant policy documents, we find a vision of general purpose, high flexibility, high utility data sharing for research and innovation.</p> </li> <li> <p>By definition, the inherent risks of these envisioned data use cases are large. If we provide the desired utility for the analyst, we must reveal a lot of information that can be misused to infer private information.</p> </li> <li> <p>This is the fundamental trade-off of privacy-preserving data publishing.</p> </li> </ul> <p><strong>Due to the fundamental trade-offs of privacy-preserving data sharing, purely technological solutions might never achieve the envisioned high utility data sharing for secondary purposes under strong privacy protections as envisioned by many strategic policy documents.</strong></p> <ul> <li> <p>No privacy-enhancing technology <em>that gives the desired utilit</em>y can eliminate the inherent risks of a data use case.</p> </li> <li> <p>Thus, privacy-enhancing technologies that produce (micro)data useful for secondary data use, such as research and innovation, do not provide strong privacy protections and, vice versa, privacy-enhancing technologies that provide strong privacy protections do not fulfil the utility requirements.</p> </li> </ul> <p>To demonstrate this take the example of synthetic data:</p> <ul> <li> <p>Assume that synthetic data fulfils the promise of “drop-in replacement for the original data” or “preserve information richness”. This implies that the synthetic dataset shared must preserve information for many possible “advanced analytics task”, incl. things like medical anomaly detection. If an analyst can successfully use this information for valid statistical analysis, a privacy adversary can successfully use this information to infer accurate information about individual outliers in the data. Thus, the synthetic data does not provide strong privacy protection.</p> </li> <li> <p>Assume that the synthetic data fulfils the promise of strong privacy, i.e., fulfil the formal guarantees of differential privacy. By definition, this dataset may not preserve any (accurate) information about individual records, such as outliers. Thus, any analysis that requires this statistical information will be inaccurate. Thus, the synthetic data does not fulfil the promise of “drop-in replacement” for the original data.</p> </li> </ul> <p>If you are interested in hearing the full reasoning behind these messages, feel fre to get in contact. I’m always happy to connect and give this talk to a wider audience.</p>]]></content><author><name></name></author><category term="privacy,"/><category term="policy"/><summary type="html"><![CDATA[Lecture at the Brussels Privacy Hub Global Summer Academy for Privacy Law 2024]]></summary></entry><entry><title type="html">Rethinking Data</title><link href="https://reslbesl.github.io/reslbesl/blog/2023/cpdp/" rel="alternate" type="text/html" title="Rethinking Data"/><published>2023-05-24T00:00:00+00:00</published><updated>2023-05-24T00:00:00+00:00</updated><id>https://reslbesl.github.io/reslbesl/blog/2023/cpdp</id><content type="html" xml:base="https://reslbesl.github.io/reslbesl/blog/2023/cpdp/"><![CDATA[<p>It was a great pleasure to discuss about the future of data use and regulation as part of the panel titled “Looking beyond the EU data strategy: where next for data use and regulation?” at CPDP 2023. The panel was organised by Valentina Pavel, a senior researcher at the Ada Lovelace Institute (UK) and featured with Adriana Nugter, Katarzyna Szymielewicz, and Inge Graef three other impressive women in the field.</p> <p>The aim of this panel was to reflect critically on fundamental questions that are left unaddressed by existing data &amp; AI regulation in the EU, as well as on potential opportunities that can prepare the ground for more ambitious transformations in data-driven systems that benefit people and society (including potential ideas that might inspire the new European Commission). The discussion built on the “Rethinking data and rebalancing digital power” report published by the Ada Lovelace Institute last year.</p> <p>Some of the input that I gave to the discussion was about two main opportunities and challenges that I see following from the EU digital package of regulation:</p> <p>First, I see a big opportunity for a paradigm change from “release then evaluate” to “evaluate then release” . What we are currently experiencing with the new generation of generative models is once more all the issues caused when technology companies are free to release their newest product into the world without having to first attest that the potential harms caused by the models are controlable and weighed off by its benefits for society at large. With the new round of regulations there is a chance to change that and move towards a slow release model where new technolgies only become avaialble once we have found the right means to assess their potential risks.</p> <p>The second opportunity I see is to challenge the assumption that in data-driven technologies everything must be possible and should be done or that more data sharing is always better. Instead, I hope, that when we build the new digital infrastructures envisioned in the re-thinking data report or propose new legislation, we will manage to avoid the mistakes of the past where we often assumed that some technological silver bullet solutions will save us and, for example, resolve any trade-offs between privacy and utility we have to make. Going forward we should be very open and explicit about the trade-offs we have to make and acknowledge when technological safeguards cannot provide any meaningful risk mitigation while also implementing the functionality that we want. I think it’s then up to us to decide whether we accept the risks of relying only on procedural controls or to maybe say no, we do not want that and the trade-off is not worth it.</p> <p>It was a fun panel to be part of and Valentina did such a great job at moderating it.</p> <p><em>Picture by CPDP</em></p>]]></content><author><name></name></author><category term="privacy"/><category term="policy"/><summary type="html"><![CDATA[Panel discussion at CPDP 2023]]></summary></entry><entry><title type="html">Synthetic Data - Anonymisation Groundhog Day</title><link href="https://reslbesl.github.io/reslbesl/blog/2021/synthetic/" rel="alternate" type="text/html" title="Synthetic Data - Anonymisation Groundhog Day"/><published>2021-09-22T00:00:00+00:00</published><updated>2021-09-22T00:00:00+00:00</updated><id>https://reslbesl.github.io/reslbesl/blog/2021/synthetic</id><content type="html" xml:base="https://reslbesl.github.io/reslbesl/blog/2021/synthetic/"><![CDATA[<p>I’m very happy to share that our work that evaluates synthetic data sharing as a privacy mechanism titled “Synthetic Data- Anonymisation Groundhog Day” was accepted for publication at USENIX Security’22. The final version can be found <a href="https://www.usenix.org/system/files/sec22-stadler.pdf">here</a>. Below, I shortly summarise our key findings and conclusions.</p> <h2 id="a-short-summary">A short summary</h2> <p><strong>What motivates our work?</strong> Both, practitioners and researchers, often present synthetic data as The Privacy Solution that addresses the shortcomings of traditional anonymization and enables data sharing with a much better privacy-utility tradeoff than row-level sanitisation.</p> <p>But so far there is no evidence that this claim actually holds true. There is no rigorous analysis that shows that synthetic data indeed provides better privacy protection at a lower cost in utility than traditional sanitisation.</p> <p><strong>What we did?</strong> We designed and implemented an evaluation framework that allows us to quantitatively assess this claim. We evaluated a wide variety of standard and differentially private generative models and assessed whether they indeed provide better privacy-utility tradeoffs for high-dimensional data sharing than traditional sanitisation.</p> <p><strong>What we found?</strong> In essence, we find that synthetic data is far from the magic silver bullet it is promised to be. The tradeoff between preserving fine-grained statistical patterns and protecting the privacy of outlier records in high-dimensional data releases remains.</p> <p>Synthetic data sampled from models without explicit privacy protection leaves outliers vulnerable to linkage attacks. Differentially private model training increases the privacy gain of synthetic data sharing but does not retain sufficient data utility for many use cases.</p> <p>For instance, in the graph below on the left side we compare the privacy gain of (differentially private) synthetic data publishing to that of traditional sanitisation. We can see that while sanitisation, as expected, leaves some outlier records vulnerable to privacy attacks (privacy gain far below 1), synthetic data provides a higher gain for most records and differentially private synthetic data further improves this gain. However, as we show on the right, this comes at a significant cost in utility. If we use the (differentially private) synthetic data instead of the raw or sanitised data to train a machine learning model, the model’s performance drops substantially.</p> <p><img src="assets/img/put.png" alt="put"/></p> <p>Worse, not only does synthetic data fail to improve over the privacy-utility tradeoff of traditional anonymisation, we also show why it is actually much less suitable as a privacy mechanism: The privacy gain for individuals and the utility loss for specific use cases is not predictable. This property make synthetic data a pretty bad privacy mechanism.</p> <p><strong>What else did we learn?</strong> Our empirical evaluation of two differentially private models frequently used in practice revealed that their existing implementations violated important theoretical assumptions of the differential privacy model. Therefore, both models did not provide the formal guarantees they promised to and left outlier records vulnerable to privacy attacks.</p> <p>We suggest a way to avoid this leakage but observe that if a model’s implementation meets all assumptions and provides strict privacy guarantees, its utility decreases substantially. This is not unexpected given all the previous literature that shows that privacy for sparse, high-dimensional data is hard. A great summary of this fundamental tradeoff can be found in this excellent <a href="https://www.cs.princeton.edu/~arvindn/publications/de-anonymization-retrospective.pdf">blog post</a> by Arvind Narayanan and Vitaly Shmatikov.</p> <p>This and the rest of our findings highlight once more the importance of empirical evaluations for building privacy-preserving systems. To help others with this, we publish our framework as an (hopefully) easy-to-use <a href="https://github.com/spring-epfl/synthetic_data_release">open-source library</a> and have started multiple collabs to help people use it.</p> <h2 id="key-takeaways">Key takeaways</h2> <p>Synthetic data is far from the holy-grail of privacy-preserving data publishing. The promise that it provides a higher gain in privacy than traditional sanitisation at a lower cost in utility rarely holds true.</p> <p>On top of that, synthetic data, in comparison to deterministic sanitisation procedures, does not allow us to predict what signals will preserved and what information will be lost. This leads to a highly variable privacy gain and unpredictable utility loss.</p> <p><em>Photo by Pixabay from Pexels</em></p>]]></content><author><name></name></author><category term="privacy"/><summary type="html"><![CDATA[The false promise of synthetic data as the holy grail of privacy-preserving data publishing]]></summary></entry><entry><title type="html">Die Risiken des LUCA Systems</title><link href="https://reslbesl.github.io/reslbesl/blog/2021/luca-de/" rel="alternate" type="text/html" title="Die Risiken des LUCA Systems"/><published>2021-04-02T00:00:00+00:00</published><updated>2021-04-02T00:00:00+00:00</updated><id>https://reslbesl.github.io/reslbesl/blog/2021/luca-de</id><content type="html" xml:base="https://reslbesl.github.io/reslbesl/blog/2021/luca-de/"><![CDATA[<p>Über die letzten Tage haben wir eine <a href="https://arxiv.org/pdf/2103.11958">Analyse</a> der möglichen Nachteile, die ein breiter Einsatz des LUCA Systems für Individuen, Gruppen und Veranstalter mit sich bringen könnte, verfasst.</p> <h2 id="eine-kurze-zusammenfassung">Eine kurze Zusammenfassung</h2> <p>Die Luca-App sammelt und speichert eine große Anzahl an sensiblen Daten über ihre Nutzer und die registrierten Veranstaltungsorte. Der größte Teil dieser Daten ist aber gar nicht nötig um den Gesundheitsämtern die Kontaktverfolgung zu erleichtern. Dazu bräuchte es keinerlei zentrale Datenspeicher. Das Risiko, das das LUCA-System für Nutzer und Veranstalter darstellt, scheint daher in keinem Verhältnis zu ihrem möglichen Nutzen zu stehen.</p> <p>Den Betreibern des LUCA-Systems ist es z.B. möglich in Echtzeit zu beobachten was bei Veranstaltungen vorsichgeht (Anzahl der Teilnehmer, Dauer des Besuches, Anzahl der positiven Fälle) und pseudonimisierte Daten über Nutzer und ihr Verhalten zu sammeln. Die gesammelten Daten sind nicht nur für kommerzielle Zwecke wertvoll sondern könnten auch für die Überwachung bestimmter Gruppen auf Basis ihrer politischen oder religiösen Zugehörigkeit eingesetzt werden.</p> <p>Dies ist besonders besorgniserregend, wenn man bedenkt, dass die App auch in Moscheen und Synagogen und für private Veranstaltungen, wie Familienfeiern, zum Einsatz kommen soll. Die <a href="https://www.regierung-mv.de/static/Regierungsportal/Portalredaktion/Inhalte/Corona/Corona-Verordnung.pdf">Corona-Landesverordnung</a> des Landes Mecklenburg-Vorpommern z.B. macht die Nutzung der Luca-App zur Auflage für Moscheen, Synagogen, Kapellen, für Sitzungen von kommunalen Gremien, Parteien, Verbänden und Vereinen, und für private Zusammenkünfte.</p> <p>Zudem hängt die Einhaltung des Datenschutzes des gesamten Systems allein vom Verhalten des Luca-Servers ab. Jeder der Zugang zu diesem zentralen Server erlangt, durch Zwang, illegales Handeln oder rechtliche Befugnis, hat direkten Zugrif auf hochsensible Daten: Wer war wo wielange mit wem?</p> <p>Die meisten Risiken entstehen durch die zentrale Architektur des Systems und sind nicht einfach zu beheben. Manche Schwachstellen könnten durch Verschlüsselungstechniken behoben werden. Das große Problem der zentralen Machtstellung des Lucaservers bleibt.</p> <p><em>Photo by Scott Webb from Pexels</em></p>]]></content><author><name></name></author><summary type="html"><![CDATA[Über die letzten Tage haben wir eine Analyse der möglichen Nachteile, die ein breiter Einsatz des LUCA Systems für Individuen, Gruppen und Veranstalter mit sich bringen könnte, verfasst.]]></summary></entry><entry><title type="html">Analysis of the Potential Harms in the LUCA Tracing System</title><link href="https://reslbesl.github.io/reslbesl/blog/2021/luca/" rel="alternate" type="text/html" title="Analysis of the Potential Harms in the LUCA Tracing System"/><published>2021-04-02T00:00:00+00:00</published><updated>2021-04-02T00:00:00+00:00</updated><id>https://reslbesl.github.io/reslbesl/blog/2021/luca</id><content type="html" xml:base="https://reslbesl.github.io/reslbesl/blog/2021/luca/"><![CDATA[<p>We recently published a preliminary analysis of the potential harms of the LUCA tracing system. The LUCA system is a digital presence tracing system designed to provide health departments with the contact information necessary to alert individuals who have visited a location at the same time as a SARS-CoV-2-positive person. Multiple regional health departments in Germany had recently announced their plans to deploy the LUCA system for the purpose of presence tracing.</p> <p>In the meantime, some German states have inacted <a href="https://www.regierung-mv.de/static/Regierungsportal/Portalredaktion/Inhalte/Corona/Corona-Verordnung.pdf">laws</a> that mandate venues from bars and restaurants to churches, therapists, and political party gatherings to use the LUCA system to record the presence of individuals.</p> <p>Our analysis of the system’s design uncovers major concerns about its potential harms to individuals, communities, and venues.</p> <h2 id="a-short-summary-of-our-findings">A short summary of our findings</h2> <p><strong>Information leakage by design.</strong> In its normal mode of operation, the LUCA Backend Server collects and processes a large amount of sensitive information about venues and individuals. By design, the Luca Backend Server can learn how many individuals are currently visiting a registered venue, how long they stay, and how many people who visited a venue in the past tested positive afterwards. The central server can link visits of anonymous users through network metadata which might lead to the re-identification of users based on their location visits. These inferences do not require the adversary to actively modify the system’s operational information flows or circumvent its protection mechanisms. The LUCA service operator (or any entity that compromises, coerces, or subpoenas the Luca Backend Server) can cause any of these harms without being detected.</p> <p><strong>High risk of abuse due to centralisation of trust.</strong> The Luca system centralises trust in a single powerful entity, the party operating the Luca Backend Server. If this central entity acts maliciously, or is compromised, or coerced, the server could gain full access to any individual user’s contact data and location history. The current system does not provide any technical safeguard against arbitrary access to this information in case of misbehaviour. Its security concept solely relies on procedural controls and requires full trust in the Luca service operator to follow the protocols faithfully.</p> <h2 id="media-coverage-in-german">Media coverage (in German)</h2> <p>I gave an interview to the German national <a href="https://www.zdf.de/nachrichten/heute-journal/pilotprojekt-weimar-100.html">news</a> and one of the public broadcasting <a href="https://www.radioeins.de/programm/sendungen/der_schoene_morgen/_/luca-app--sinn-und-nutzen-der-app.html">stations</a> about our findings. A great <a href="https://www.zeit.de/digital/datenschutz/2021-03/corona-app-luca-kontaktverfolgung-einsatz-umstritten-kontakte-politik-lobbyismus">article</a> by Eva Wolfangel for Die Zeit (one of the three biggest German news outlets) covers not only our analysis but also other problematic issues around the deployment of the LUCA system.</p> <p>In this long format <a href="https://www.ndr.de/nachrichten/info/podcast4808.html">podcast</a> I cover a wide range of issues relating to LUCA.</p> <p><em>Photo by Scott Webb from Pexels</em></p>]]></content><author><name></name></author><category term="privacy"/><category term="systems"/><summary type="html"><![CDATA[The risk of targeted surveillance and social stigmatisation]]></summary></entry></feed>